{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Overview of CNN Libraries\n",
    "---\n",
    "#### By: Jesse Brizzi\n",
    "#### jbrizzi@cs.stonybrook.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you want to run this code, you can use an Amazon EC2 server\n",
    "\n",
    "[set up a deep learning server](http://markus.com/install-theano-on-aws/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is a CNN?\n",
    "\n",
    "[MINST DEMO](http://cs.stanford.edu/people/karpathy/convnetjs/demo/mnist.html)\n",
    "\n",
    "![Convolution](http://deeplearning.stanford.edu/wiki/images/6/6c/Convolution_schematic.gif)\n",
    "![ReLU](http://danielnouri.org/media/kfkd/rectifier.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of current available libraries\n",
    "---\n",
    "\n",
    "**Python**\n",
    "\n",
    "1.  [Theano](http://deeplearning.net/software/theano) is a python library for defining and evaluating mathematical expressions with numerical arrays. It makes it easy to write deep learning algorithms in python. On the top of the Theano many more libraries are built.\n",
    "\n",
    "    1.  [Keras](http://keras.io/) is a minimalist, highly modular neural network library in the spirit of Torch, written in Python, that uses Theano under the hood for optimized tensor manipulation on GPU and CPU.\n",
    "\n",
    "    2.  [Pylearn2](http://deeplearning.net/software/pylearn2/) is a library that wraps a lot of models and training algorithms such as Stochastic Gradient Descent that are commonly used in Deep Learning. Its functional libraries are built on top of Theano.\n",
    "\n",
    "    3.  [Lasagne](https://github.com/Lasagne/Lasagne) is a lightweight library to build and train neural networks in Theano. It is governed by simplicity, transparency, modularity, pragmatism , focus and restraint principles.\n",
    "\n",
    "    4.  [Blocks](https://github.com/mila-udem/blocks) a framework that helps you build neural network models on top of Theano.\n",
    "\n",
    "2.  [Caffe](http://caffe.berkeleyvision.org/) is a deep learning framework made with expression, speed, and modularity in mind. It is developed by the Berkeley Vision and Learning Center (BVLC) and by community contributors. Google's [DeepDream](http://venturebeat.com/2015/07/01/google-open-sources-its-software-for-making-trippy-images-with-deep-learning/) is based on Caffe Framework. This framework is a BSD-licensed C++ library with Python Interface.\n",
    "\n",
    "3.  [_nolearn_](https://github.com/dnouri/nolearn) contains a number of wrappers and abstractions around existing neural network libraries, most notably [Lasagne](http://lasagne.readthedocs.org/), along with a few machine learning utility modules.\n",
    "\n",
    "4.  [Gensim](http://radimrehurek.com/gensim/) is deep learning toolkit implemented in python programming language intended for handling large text collections, using efficient algorithms.\n",
    "\n",
    "5.  [Chainer](http://chainer.org/) bridge the gap between algorithms and implementations of deep learning. Its powerful, flexible and intuitive and is considered as the [flexible framework](http://www.slideshare.net/beam2d/introduction-to-chainer-a-flexible-framework-for-deep-learning) for Deep Learning.\n",
    "\n",
    "6.  [deepnet](https://github.com/nitishsrivastava/deepnet) is a GPU-based python implementation of deep learning algorithms like Feed-forward Neural Nets, Restricted Boltzmann Machines, Deep Belief Nets, Autoencoders, Deep Boltzmann Machines and Convolutional Neural Nets.\n",
    "\n",
    "7.  [Hebel](https://github.com/hannes-brt/hebel) is a library for deep learning with neural networks in Python using GPU acceleration with CUDA through PyCUDA. It implements the most important types of neural network models and offers a variety of different activation functions and training methods such as momentum, Nesterov momentum, dropout, and early stopping.\n",
    "\n",
    "8.  [CXXNET](https://github.com/dmlc/cxxnet) is fast, concise, distributed deep learning framework based on MShadow. It is a lightweight and easy extensible C++/CUDA neural network toolkit with friendly Python/Matlab interface for training and prediction.\n",
    "\n",
    "9.  [DeepPy](https://github.com/andersbll/deeppy) is a Pythonic deep learning framework built on top of NumPy.\n",
    "\n",
    "10.  [DeepLearning](https://github.com/vishwa-raman/DeepLearning) is deep learning library, developed with C++ and python.\n",
    "\n",
    "11.  [Neon](https://github.com/NervanaSystems/neon) is Nervana's Python based Deep Learning framework.\n",
    "\n",
    "**Matlab**\n",
    "\n",
    "1.  [ConvNet](https://github.com/sdemyanov/ConvNet) Convolutional neural net is a type of deep learning classification algorithms, that can learn useful features from raw data by themselves and is performed by tuning its weighs.\n",
    "\n",
    "2.  [DeepLearnToolBox](https://github.com/rasmusbergpalm/DeepLearnToolbox) is a matlab/octave toolbox for deep learning and includes Deep Belief Nets, Stacked Autoencoders, convolutional neural nets.\n",
    "\n",
    "3.  [cuda-convnet](https://code.google.com/p/cuda-convnet/) is a fast C++/CUDA implementation of convolutional (or more generally, feed-forward) neural networks. It can model arbitrary layer connectivity and network depth. Any directed acyclic graph of layers will do. Training is done using the backpropagation algorithm.\n",
    "\n",
    "4.  [MatConvNet](http://www.vlfeat.org/matconvnet/ \"MatConvNet\") is a MATLAB toolbox implementing Convolutional Neural Networks (CNNs) for computer vision applications. It is simple, efficient, and can run and learn state-of-the-art CNNs.\n",
    "\n",
    "5.  [MatCaffe](http://caffe.berkeleyvision.org/) is a deep learning framework made with expression, speed, and modularity in mind. It is developed by the Berkeley Vision and Learning Center (BVLC) and by community contributors. Google's [DeepDream](http://venturebeat.com/2015/07/01/google-open-sources-its-software-for-making-trippy-images-with-deep-learning/) is based on Caffe Framework. This framework is a BSD-licensed C++ library with Python Interface.\n",
    "\n",
    "**CPP**\n",
    "\n",
    "1.  [eblearn](http://eblearn.sourceforge.net/index.shtml) is an open-source C++ library of machine learning by New York University’s machine learning lab, led by Yann LeCun. In particular, implementations of convolutional neural networks with energy-based models along with a GUI, demos and tutorials.\n",
    "\n",
    "2.  [SINGA](http://www.comp.nus.edu.sg/~dbsystem/singa/) is designed to be general to implement the distributed training algorithms of existing systems. It is supported by Apache Software Foundation.\n",
    "\n",
    "3.  NVIDIA [DIGITS](https://developer.nvidia.com/digits) is a new system for developing, training and visualizing deep neural networks. It puts the power of deep learning into an intuitive browser-based interface, so that data scientists and researchers can quickly design the best DNN for their data using real-time network behavior visualization.\n",
    "\n",
    "4.  [Intel® Deep Learning Framework](https://01.org/intel-deep-learning-framework) provides a unified framework for Intel® platforms accelerating Deep Convolutional Neural Networks.\n",
    "\n",
    "**Java**\n",
    "\n",
    "1.  [N-Dimensional Arrays for Java](http://nd4j.org/) (ND4J)is scientific computing libraries for the JVM. They are meant to be used in production environments, which means routines are designed to run fast with minimum RAM requirements.\n",
    "\n",
    "2.  [Deeplearning4j](http://deeplearning4j.org/) is the first commercial-grade, open-source, distributed deep-learning library written for Java and Scala. It is designed to be used in business environments, rather than as a research tool.\n",
    "\n",
    "3.  [Encog](http://www.heatonresearch.com/encog) is an advanced machine learning framework which supports Support Vector Machines,Artificial Neural Networks, Genetic Programming, Bayesian Networks, Hidden Markov Models, Genetic Programming and Genetic Algorithms are supported.\n",
    "\n",
    "**JavaScript**\n",
    "\n",
    "1.  [Convnet.js](http://cs.stanford.edu/people/karpathy/convnetjs/) is a Javascript library for training Deep Learning models (mainly Neural Networks) entirely in a browser. No software requirements, no compilers, no installations, no GPUs, no sweat.\n",
    "\n",
    "**Lua**\n",
    "\n",
    "1.  [Torch](http://torch.ch/) is a scientific computing framework with wide support for machine learning algorithms. It is easy to use and efficient, fast scripting language, LuaJIT, and an underlying C/CUDA implementation. Torch is based on Lua programming language.\n",
    "\n",
    "**Julia**\n",
    "\n",
    "1.  [Mocha](https://github.com/pluskid/Mocha.jl) is a Deep Learning framework for Julia, inspired by the C++ framework Caffe. Efficient implementations of general stochastic gradient solvers and common layers in Mocha could be used to train deep / shallow (convolutional) neural networks, with (optional) unsupervised pre-training via (stacked) auto-encoders. Its best feature include Modular architecture, High-level Interface, portability with speed, compatibility and many more.\n",
    "\n",
    "**Lisp**\n",
    "\n",
    "1.  [Lush(Lisp Universal Shell)](http://lush.sourceforge.net/) is an object-oriented programming language designed for researchers, experimenters, and engineers interested in large-scale numerical and graphic applications. It comes with rich set of deep learning libraries as a part of machine learning libraries.\n",
    "\n",
    "**Haskell**\n",
    "\n",
    "1.  [DNNGraph](https://github.com/ajtulloch/dnngraph) is a deep neural network model generation DSL in Haskell.\n",
    "\n",
    "**.NET**\n",
    "\n",
    "1.  [Accord.NET](http://accord-framework.net/ \"Accord.NET\") is a .NET machine learning framework combined with audio and image processing libraries completely written in C#. It is a complete framework for building production-grade computer vision, computer audition, signal processing and statistics applications\n",
    "\n",
    "**R**\n",
    "\n",
    "1.  [darch](http://cran.um.ac.ir/web/packages/darch/index.html \"darch\") package can be used for generating neural networks with many layers (deep architectures). Training methods includes a pre training with the contrastive divergence method and a fine tuning with common known training algorithms like backpropagation or conjugate gradient.\n",
    "2.  [deepnet](https://cran.r-project.org/web/packages/deepnet/index.html \"deepnet\") implements some deep learning architectures and neural network algorithms, including BP,RBM,DBN,Deep autoencoder and so on.\n",
    "\n",
    "\n",
    "\n",
    "source [http://www.teglor.com/b/deep-learning-libraries-language-cm569/](http://www.teglor.com/b/deep-learning-libraries-language-cm569/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convnet-benchmarks\n",
    "---\n",
    "\n",
    "Easy benchmarking of all public open-source implementations of convnets.\n",
    "A summary is provided in the section below.\n",
    "\n",
    "Machine: `6-core Intel Core i7-5930K CPU @ 3.50GHz` + `NVIDIA Titan X` + `Ubuntu 14.04 x86_64`\n",
    "\n",
    "## Imagenet Winners Benchmarking\n",
    "Time for a full forward + backward pass. I average my times over 10 runs. Ignoring dropout and softmax layers.\n",
    "\n",
    "**[AlexNet (One Weird Trick paper)](https://code.google.com/p/cuda-convnet2/source/browse/layers/layers-imagenet-1gpu.cfg)** - Input 128x3x224x224\n",
    "\n",
    "| Library         | Class                                                                                                                | Time (ms)  | forward (ms) | backward (ms) |\n",
    "|:------------------------:|:-----------------------------------------------------------------------------------------------------------:| ----------:| ------------:| -------------:|\n",
    "| **Nervana-fp16**    | [ConvLayer](https://github.com/soumith/convnet-benchmarks/blob/master/nervana/README.md)                    |   **92**   |  **29**      |    **62**     |\n",
    "| CuDNN[R3]-fp16      | [cudnn.SpatialConvolution](https://github.com/soumith/cudnn.torch/blob/master/SpatialConvolution.lua)       |      96    |  30          |   66          |\n",
    "| CuDNN[R3]-fp32      | [cudnn.SpatialConvolution](https://github.com/soumith/cudnn.torch/blob/master/SpatialConvolution.lua)       |      96    |  32          |   64          |\n",
    "| Nervana-fp32        | [ConvLayer](https://github.com/soumith/convnet-benchmarks/blob/master/nervana/README.md)                    |      101   |  32          |    69         |\n",
    "| fbfft                    | [fbnn.SpatialConvolution](https://github.com/facebook/fbcunn/tree/master/src/fft)                           |      104   |  31          |    72         |\n",
    "| cudaconvnet2*            | [ConvLayer](https://github.com/soumith/cuda-convnet2.torch/blob/master/cudaconv3/src/filter_acts.cu)        |      177   |  42          |   135         |\n",
    "| CuDNN[R2] *             | [cudnn.SpatialConvolution](https://github.com/soumith/cudnn.torch/blob/master/SpatialConvolution.lua)       |      231   |  70          |   161         |\n",
    "| Caffe (native)           | [ConvolutionLayer](https://github.com/BVLC/caffe/blob/master/src/caffe/layers/conv_layer.cu)                |      324   | 121          |   203         |\n",
    "| Torch-7 (native)         | [SpatialConvolutionMM](https://github.com/torch/cunn/blob/master/SpatialConvolutionMM.cu)                   |      342   | 132          |   210         |\n",
    "| CL-nn (Torch)            | [SpatialConvolutionMM](https://github.com/hughperkins/clnn/blob/master/SpatialConvolutionMM.cl)             |      963   | 388          |   574         |\n",
    "| Caffe-CLGreenTea         | [ConvolutionLayer](https://github.com/naibaf7/caffe)             |      1442   | 210          |   1232         |\n",
    "\n",
    "**[Overfeat [fast]](http://arxiv.org/abs/1312.6229)** - Input 128x3x231x231\n",
    "\n",
    "| Library                  | Class                                                                                                                    | Time (ms)         | forward (ms)            | backward (ms)            |\n",
    "|:------------------------:|:------------------------------------------------------------------------------------------------------------------------:| -----------------:| -----------------------:| ------------------------:|\n",
    "| **CuDNN[R3]-fp16**       | [cudnn.SpatialConvolution](https://github.com/soumith/cudnn.torch/blob/master/SpatialConvolution.lua)                    |         **313**       |  **107**                    |  **206**             |\n",
    "| CuDNN[R3]-fp32       | [cudnn.SpatialConvolution](https://github.com/soumith/cudnn.torch/blob/master/SpatialConvolution.lua)                    |         326       |  113                    |   213                    |\n",
    "| fbfft                    | [SpatialConvolutionCuFFT](https://github.com/facebook/fbcunn/tree/master/src/fft)                                        |         342       |  114                    |   227                    |\n",
    "| Nervana-fp16          | [ConvLayer](https://github.com/soumith/convnet-benchmarks/blob/master/nervana/README.md)                                 |         355       |  112                    |   242                    |\n",
    "| Nervana-fp32            | [ConvLayer](https://github.com/soumith/convnet-benchmarks/blob/master/nervana/README.md)                                 |         398       |  124                    |   273                    |\n",
    "| cudaconvnet2*            | [ConvLayer](https://github.com/soumith/cuda-convnet2.torch/blob/master/cudaconv3/src/filter_acts.cu)                     |         723       |  176                    |   547                    |\n",
    "| CuDNN[R2] *             | [cudnn.SpatialConvolution](https://github.com/soumith/cudnn.torch/blob/master/SpatialConvolution.lua)                    |         810       |  234                    |   576                    |\n",
    "| Caffe                    | [ConvolutionLayer](https://github.com/BVLC/caffe/blob/master/src/caffe/layers/conv_layer.cu)                             |         823       |  355                    |   468                    |\n",
    "| Torch-7 (native)         | [SpatialConvolutionMM](https://github.com/torch/cunn/blob/master/SpatialConvolutionMM.cu)                                |         878       |  379                    |   499                    |\n",
    "| CL-nn (Torch)            | [SpatialConvolutionMM](https://github.com/hughperkins/clnn/blob/master/SpatialConvolutionMM.cl)                          |         963       |  388                    |   574                    |\n",
    "| Caffe-CLGreenTea         | [ConvolutionLayer](https://github.com/naibaf7/caffe)             |      2857   | 616          |   2240         |\n",
    "\n",
    "**[OxfordNet [Model-A]](http://arxiv.org/abs/1409.1556/)** - Input 64x3x224x224\n",
    "\n",
    "| Library                  | Class                                                                                                                    | Time (ms)         | forward (ms)            | backward (ms)            |\n",
    "|:------------------------:|:------------------------------------------------------------------------------------------------------------------------:| -----------------:| -----------------------:| ------------------------:|\n",
    "| **Nervana-fp16**    | [ConvLayer](https://github.com/soumith/convnet-benchmarks/blob/master/nervana/README.md)                                 |    **529**        |  **167**                |   **362**                |\n",
    "| Nervana-fp32        | [ConvLayer](https://github.com/soumith/convnet-benchmarks/blob/master/nervana/README.md)                                 |        590        |  180                    |   410                    |\n",
    "| CuDNN[R3]-fp16      | [cudnn.SpatialConvolution](https://github.com/soumith/cudnn.torch/blob/master/SpatialConvolution.lua)                    |       615         |  179                    |   436                    |\n",
    "| CuDNN[R3]-fp32      | [cudnn.SpatialConvolution](https://github.com/soumith/cudnn.torch/blob/master/SpatialConvolution.lua)                    |       615         |  196                    |   418                    |\n",
    "| fbfft                    | [SpatialConvolutionCuFFT](https://github.com/facebook/fbcunn/tree/master/src/fft)                                        |       1092        |  355                    |   737                    |\n",
    "| cudaconvnet2*            | [ConvLayer](https://github.com/soumith/cuda-convnet2.torch/blob/master/cudaconv3/src/filter_acts.cu)                     |       1229        |  408                    |   821                    |\n",
    "| CuDNN[R2] *             | [cudnn.SpatialConvolution](https://github.com/soumith/cudnn.torch/blob/master/SpatialConvolution.lua)                    |       1099        |  342                    |   757                    |\n",
    "| Caffe                    | [ConvolutionLayer](https://github.com/BVLC/caffe/blob/master/src/caffe/layers/conv_layer.cu)                             |       1068        |  323                    |   745                    |\n",
    "| Torch-7 (native)         | [SpatialConvolutionMM](https://github.com/torch/cunn/blob/master/SpatialConvolutionMM.cu)                                |       1105        |  350                    |   755                    |\n",
    "| CL-nn (Torch)            | [SpatialConvolutionMM](https://github.com/hughperkins/clnn/blob/master/SpatialConvolutionMM.cl)                          |       3437        |  875                    |   2562                   |\n",
    "| Caffe-CLGreenTea         | [ConvolutionLayer](https://github.com/naibaf7/caffe)             |      5620   | 988          |   4632         |\n",
    "\n",
    "**[GoogleNet V1](http://research.google.com/pubs/pub43022.html)** - Input 128x3x224x224\n",
    "\n",
    "| Library                  | Class                                                                                                                    | Time (ms)         | forward (ms)            | backward (ms)            |\n",
    "|:------------------------:|:------------------------------------------------------------------------------------------------------------------------:| -----------------:| -----------------------:| ------------------------:|\n",
    "| **Nervana-fp16**    | [ConvLayer](https://github.com/soumith/convnet-benchmarks/blob/master/nervana/README.md)                                 |    **283**        |  **85**                 |   **197**                |\n",
    "| Nervana-fp32        | [ConvLayer](https://github.com/soumith/convnet-benchmarks/blob/master/nervana/README.md)                                 |        322        |  90                     |   232                    |\n",
    "| CuDNN[R3]-fp32       | [cudnn.SpatialConvolution](https://github.com/soumith/cudnn.torch/blob/master/SpatialConvolution.lua)                    |       431         |  117                    |   313                    |\n",
    "| CuDNN[R3]-fp16       | [cudnn.SpatialConvolution](https://github.com/soumith/cudnn.torch/blob/master/SpatialConvolution.lua)                    |       501         |  109                    |   392                    |\n",
    "| Caffe                    | [ConvolutionLayer](https://github.com/BVLC/caffe/blob/master/src/caffe/layers/conv_layer.cu)                             |       1935        |  786                    |   1148                   |\n",
    "| CL-nn (Torch)            | [SpatialConvolutionMM](https://github.com/hughperkins/clnn/blob/master/SpatialConvolutionMM.cl)                          |       7016        |  3027                   |   3988                   |\n",
    "| Caffe-CLGreenTea         | [ConvolutionLayer](https://github.com/naibaf7/caffe)             |      9462   | 746          |   8716         |\n",
    "\n",
    "source [https://github.com/soumith/convnet-benchmarks](https://github.com/soumith/convnet-benchmarks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# The Big 3\n",
    "\n",
    "**Caffe**\n",
    "- C++/YAML with python and matlab interfaces\n",
    "- Only can implement CNN's\n",
    "- Huge community and active community\n",
    "- Most computer vision research uses Caffe\n",
    "- Can implement your own layers in c++/cuda/python\n",
    "- Colaberates with Nvidia, first to use new CudNN libraries\n",
    "- Have to define network structure in YAML\n",
    "- OSX and Linux\n",
    "\n",
    "**Theano**\n",
    "- Python library\n",
    "- Built around the Scypy/Numpy evironment\n",
    "- General Machine Learning Library\n",
    "- Outside of computer vision, most used\n",
    "- Define net structure in python\n",
    "- Very low level, but has a lot of useful high level libraries \n",
    "    - NoLearn\n",
    "    - Lasagne\n",
    "- OSX, Linux, and Windows\n",
    "\n",
    "**Torch**\n",
    "- Lua Library\n",
    "- Used by Google and Facebook\n",
    "- Very low level\n",
    "- OSX and Linux\n",
    "\n",
    "\n",
    "\n",
    "## My advice, USE UBUNTU 14.04 LTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What GPU Do I Want?\n",
    "\n",
    "|   GPU Name  |  Cost | Memory |   Power Requirement  | Single Precision Speed |\n",
    "|:-----------:|:-----:|:------:|:--------------------:|:----------------------:|\n",
    "|  GTX 750ti  |  \\$150 |   2GB  |     55w - No PCIe    |       1306 GFLOPS      |\n",
    "|   GTX 970   |  \\$330 |  3.5GB |    145w - 2x 6-Pin   |       3494 GFLOPS      |\n",
    "|  GTX 980ti  |  \\$650 |   6GB  | 250w - 6-Pin + 8-Pin |       5632 GFLOPS      |\n",
    "| GTX Titan X | \\$1000 |  12GB  | 250w - 6-Pin + 8-Pin |       6144 GLOPS       |\n",
    "| GTX Titan Z | \\$1550 |  6GBx2 |   375w - 2x 8-Pin    |     4061 GFLOPS x2     |\n",
    "\n",
    "\n",
    "## Tesla? AMD? Titan Black?\n",
    "- CNN and nural net libraries all use CUDA as thier back end and only depend on 32 bit single percision (or 16 bit half) accuracy.\n",
    "- AMD cards do not support CUDA\n",
    "- Tesla GPU's and older Nvidia GPU's (like the Titan, Titan Black, and 700 series) use older architecture that focus more on 64 bit double percision performance, and have poor 32 bit performance for the cost. \n",
    "    - The GTX 750ti is an exception as it uses the newer architecture\n",
    "    - The Titan Z is only useful if you need to fit 2 GPU's in a computer with only 1 open PCIe slot. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can My computer use the GPU?\n",
    "\n",
    "### An open PCIe x16 slot\n",
    "![pcie slots](https://upload.wikimedia.org/wikipedia/commons/0/0c/PCI_und_PCIe_Slots.jpg)\n",
    "\n",
    "### A Power supply with enough wattage and PCIe power cables\n",
    "![8 and 8 pin connectors](http://cdn.head-fi.org/5/56/56ff61d3_pcie-connectors.jpeg)\n",
    "![powersupply sticker](http://www.techpowerup.com/reviews/Corsair/GS800/images/psu_label.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful Education Links\n",
    "- [Stanford CS231n](http://cs231n.stanford.edu/)\n",
    "- [ConvnetJS Demos](http://cs.stanford.edu/people/karpathy/convnetjs/)\n",
    "- [Neural Networks and Deep Learning eBook](http://neuralnetworksanddeeplearning.com/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo Time\n",
    "[Source for this demo](http://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/)\n",
    "\n",
    "\n",
    "## Libraries I am using\n",
    "[Theano](http://deeplearning.net/software/theano/) + [Lasagne](https://github.com/Lasagne/Lasagne) + [NoLearn](https://github.com/dnouri/nolearn)\n",
    "\n",
    "\n",
    "## Data\n",
    "\n",
    "#### Facial Keypoints Detection (Kaggle)\n",
    "![face](https://kaggle2.blob.core.windows.net/competitions/kaggle/3486/logos/front_page.png)\n",
    "\n",
    "\n",
    "- The objective of this task is to predict keypoint positions on face images. \n",
    "- [Download the training and test data from here](https://www.kaggle.com/c/facial-keypoints-detection/data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from pandas.io.parsers import read_csv\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "FTRAIN = 'training.csv'\n",
    "FTEST = 'test.csv'\n",
    "\n",
    "\n",
    "def load(test=False, cols=None):\n",
    "    \"\"\"Loads data from FTEST if *test* is True, otherwise from FTRAIN.\n",
    "    Pass a list of *cols* if you're only interested in a subset of the\n",
    "    target columns.\n",
    "    \"\"\"\n",
    "    fname = FTEST if test else FTRAIN\n",
    "    df = read_csv(os.path.expanduser(fname))  # load pandas dataframe\n",
    "\n",
    "    # The Image column has pixel values separated by space; convert\n",
    "    # the values to numpy arrays:\n",
    "    df['Image'] = df['Image'].apply(lambda im: np.fromstring(im, sep=' '))\n",
    "\n",
    "    if cols:  # get a subset of columns\n",
    "        df = df[list(cols) + ['Image']]\n",
    "\n",
    "    print(df.count())  # prints the number of values for each column\n",
    "    df = df.dropna()  # drop all rows that have missing values in them\n",
    "\n",
    "    X = np.vstack(df['Image'].values) / 255.  # scale pixel values to [0, 1]\n",
    "    X = X.astype(np.float32)\n",
    "\n",
    "    if not test:  # only FTRAIN has any target columns\n",
    "        y = df[df.columns[:-1]].values\n",
    "        y = (y - 48) / 48  # scale target coordinates to [-1, 1]\n",
    "        X, y = shuffle(X, y, random_state=42)  # shuffle train data\n",
    "        y = y.astype(np.float32)\n",
    "    else:\n",
    "        y = None\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y = load()\n",
    "print(\"X.shape == {}; X.min == {:.3f}; X.max == {:.3f}\".format(\n",
    "    X.shape, X.min(), X.max()))\n",
    "print(\"y.shape == {}; y.min == {:.3f}; y.max == {:.3f}\".format(\n",
    "    y.shape, y.min(), y.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as pyplot\n",
    "%matplotlib inline \n",
    "\n",
    "def plot_sample(x, y, axis):\n",
    "    img = x.reshape(96, 96)\n",
    "    axis.imshow(img, cmap='gray')\n",
    "    axis.scatter(y[0::2] * 48 + 48, y[1::2] * 48 + 48, marker='x', s=10)\n",
    "    \n",
    "fig = pyplot.figure(figsize=(6, 6))\n",
    "fig.subplots_adjust(\n",
    "    left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "\n",
    "for i in range(16):\n",
    "    ax = fig.add_subplot(4, 4, i + 1, xticks=[], yticks=[])\n",
    "    plot_sample(X[i], y[i], ax)\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lasagne import layers\n",
    "from lasagne.updates import nesterov_momentum\n",
    "from nolearn.lasagne import NeuralNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net = NeuralNet(\n",
    "    layers=[  # three layers: one hidden layer\n",
    "        ('input', layers.InputLayer),\n",
    "        ('hidden', layers.DenseLayer),\n",
    "        ('output', layers.DenseLayer),\n",
    "        ],\n",
    "    # layer parameters:\n",
    "    input_shape=(None, 9216),  # 96x96 input pixels per batch\n",
    "    hidden_num_units=100,  # number of units in hidden layer\n",
    "    output_nonlinearity=None,  # output layer uses identity function\n",
    "    output_num_units=30,  # 30 target values\n",
    "\n",
    "    # optimization method:\n",
    "    update=nesterov_momentum,\n",
    "    update_learning_rate=0.01,\n",
    "    update_momentum=0.9,\n",
    "\n",
    "    regression=True,  # flag to indicate we're dealing with regression problem\n",
    "    max_epochs=400,  # we want to train this many epochs\n",
    "    verbose=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Optimization methods](http://i.imgur.com/s25RsOr.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output = net.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.sqrt(net.train_history_[-1][\"valid_loss\"]) * 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_loss = np.array([i[\"train_loss\"] for i in net.train_history_])\n",
    "valid_loss = np.array([i[\"valid_loss\"] for i in net.train_history_])\n",
    "pyplot.plot(train_loss, linewidth=3, label=\"train\")\n",
    "pyplot.plot(valid_loss, linewidth=3, label=\"valid\")\n",
    "pyplot.grid()\n",
    "pyplot.legend()\n",
    "pyplot.xlabel(\"epoch\")\n",
    "pyplot.ylabel(\"loss\")\n",
    "pyplot.ylim(1e-3, 1e-2)\n",
    "pyplot.yscale(\"log\")\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, _ = load(test=True)\n",
    "y_pred = net.predict(X)\n",
    "\n",
    "fig = pyplot.figure(figsize=(6, 6))\n",
    "fig.subplots_adjust(\n",
    "    left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "\n",
    "for i in range(16):\n",
    "    ax = fig.add_subplot(4, 4, i + 1, xticks=[], yticks=[])\n",
    "    plot_sample(X[i], y_pred[i], ax)\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load2d(test=False, cols=None):\n",
    "    X, y = load(test=test)\n",
    "    X = X.reshape(-1, 1, 96, 96)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net2 = NeuralNet(\n",
    "    layers=[\n",
    "        ('input', layers.InputLayer),\n",
    "        ('conv1', layers.Conv2DLayer),\n",
    "        ('pool1', layers.MaxPool2DLayer),\n",
    "        ('conv2', layers.Conv2DLayer),\n",
    "        ('pool2', layers.MaxPool2DLayer),\n",
    "        ('conv3', layers.Conv2DLayer),\n",
    "        ('pool3', layers.MaxPool2DLayer),\n",
    "        ('hidden4', layers.DenseLayer),\n",
    "        ('hidden5', layers.DenseLayer),\n",
    "        ('output', layers.DenseLayer),\n",
    "        ],\n",
    "    input_shape=(None, 1, 96, 96),\n",
    "    conv1_num_filters=32, conv1_filter_size=(3, 3), pool1_pool_size=(2, 2),\n",
    "    conv2_num_filters=64, conv2_filter_size=(2, 2), pool2_pool_size=(2, 2),\n",
    "    conv3_num_filters=128, conv3_filter_size=(2, 2), pool3_pool_size=(2, 2),\n",
    "    hidden4_num_units=500, hidden5_num_units=500,\n",
    "    output_num_units=30, output_nonlinearity=None,\n",
    "\n",
    "    update_learning_rate=0.01,\n",
    "    update_momentum=0.9,\n",
    "\n",
    "    regression=True,\n",
    "    max_epochs=1000,\n",
    "    verbose=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y = load2d()  # load 2-d data\n",
    "output = net2.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "#with open('net2.pickle', 'wb') as f: # this is for saving the trained net.\n",
    "#    pickle.dump(net2, f, -1)\n",
    "\n",
    "net2 = pickle.load(open('net2.pickle', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.sqrt(net2.train_history_[-1][\"valid_loss\"]) * 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_loss = np.array([i[\"train_loss\"] for i in net.train_history_])\n",
    "valid_loss = np.array([i[\"valid_loss\"] for i in net.train_history_])\n",
    "pyplot.plot(train_loss, linewidth=3, label=\"train\")\n",
    "pyplot.plot(valid_loss, linewidth=3, label=\"valid\")\n",
    "pyplot.grid()\n",
    "pyplot.legend()\n",
    "pyplot.xlabel(\"epoch\")\n",
    "pyplot.ylabel(\"loss\")\n",
    "pyplot.ylim(1e-3, 1e-2)\n",
    "pyplot.yscale(\"log\")\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample1 = load(test=True)[0][6:7]\n",
    "sample2 = load2d(test=True)[0][6:7]\n",
    "y_pred1 = net.predict(sample1)[0]\n",
    "y_pred2 = net2.predict(sample2)[0]\n",
    "\n",
    "fig = pyplot.figure(figsize=(6, 3))\n",
    "ax = fig.add_subplot(1, 2, 1, xticks=[], yticks=[])\n",
    "plot_sample(sample1[0], y_pred1, ax)\n",
    "ax = fig.add_subplot(1, 2, 2, xticks=[], yticks=[])\n",
    "plot_sample(sample1[0], y_pred2, ax)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
