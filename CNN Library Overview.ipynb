{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Overview of CNN Libraries\n",
    "---\n",
    "#### By: Jesse Brizzi\n",
    "#### jbrizzi@cs.stonybrook.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If you want to run this code\n",
    "\n",
    "### If you dont have a gpu\n",
    "\n",
    "[set up a deep learning server on Amazon EC2](http://markus.com/install-theano-on-aws/)\n",
    "\n",
    "### Needed Libraries\n",
    "\n",
    "- This is a Jupyter (Ipython) notebook, an interactive document that combines markup and executable python code blocks. \n",
    "- The eaisest way to install all of the needed python libraries to run this is to use the [Anaconda Scientific Python Distribution](https://store.continuum.io/cshop/anaconda/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is a CNN?\n",
    "---\n",
    "### First what is a NN?\n",
    "- A neural network is a non-linear function that is designed and operates in a similar fashion to neurons in the human brain. \n",
    "![Brain net](http://www.geeky-gadgets.com/wp-content/uploads/2011/07/Brain-epicness1.jpg)\n",
    "\n",
    "### Classic NN Structure\n",
    "- Here we have a simple feed forward neural network that\n",
    "- It has a single hidden fully connected layer\n",
    "    - The input has 3 nodes\n",
    "    - The hidden layer as 4 nodes\n",
    "    - The output has 2 nodes\n",
    "- The network is structured as a directed acyclic graph\n",
    "- It is trained using the back propigation algorithm\n",
    "![neural net](http://white.stanford.edu/teach/images/5/57/Feedforward.jpg)\n",
    "\n",
    "### What is Convolution?\n",
    "- Convolution is a mathematical term, defined as applying a function repeatedly across the output of another function. \n",
    "    - In this context it means to apply a 'filter' over an image at all possible offsets. \n",
    "![Convolution](http://deeplearning.stanford.edu/wiki/images/6/6c/Convolution_schematic.gif)\n",
    "\n",
    "### How are the combined?\n",
    "- Convolution kernels take the place of the fully connected nodes in the first few layers\n",
    "- CNN's are exelent texture classifiers\n",
    "- To shrink the amount of variables weight sharing is used\n",
    "    - This basicaly means the same kernel is used across the entire input image space\n",
    "    - Because of this CNN's are naturally translation invariant\n",
    "- The \"nodes\" change slightly in this structure\n",
    "    - Think of each pixel of the input image ( and the outputs of the convolution layers) as a node\n",
    "    - Think of each convolution kernel as a node\n",
    "        - Its not fully connected, only takes input from its window size\n",
    "    - Is applied multiple times across the entire output of the previous layer\n",
    "    - Each convolution layer will have multiple kernels that it trains\n",
    "- Subsampling is applied in CNN's\n",
    "    - normaly used just to shrink the problem space\n",
    "    - can also increase the translation invariance nature of the network\n",
    "![CNN structure](http://white.stanford.edu/teach/images/d/df/Mylenet.png)\n",
    "\n",
    "\n",
    "### Here is a useful online visual aid\n",
    "[MINST DEMO](http://cs.stanford.edu/people/karpathy/convnetjs/demo/mnist.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful Education Links\n",
    "---\n",
    "- [Stanford CS231n](http://cs231n.stanford.edu/)\n",
    "- [ConvnetJS Demos](http://cs.stanford.edu/people/karpathy/convnetjs/)\n",
    "- [Neural Networks and Deep Learning eBook](http://neuralnetworksanddeeplearning.com/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of current available libraries\n",
    "---\n",
    "\n",
    "**Python**\n",
    "\n",
    "1.  [Theano](http://deeplearning.net/software/theano) is a python library for defining and evaluating mathematical expressions with numerical arrays. It makes it easy to write deep learning algorithms in python. On the top of the Theano many more libraries are built.\n",
    "\n",
    "    1.  [Keras](http://keras.io/) is a minimalist, highly modular neural network library in the spirit of Torch, written in Python, that uses Theano under the hood for optimized tensor manipulation on GPU and CPU.\n",
    "\n",
    "    2.  [Pylearn2](http://deeplearning.net/software/pylearn2/) is a library that wraps a lot of models and training algorithms such as Stochastic Gradient Descent that are commonly used in Deep Learning. Its functional libraries are built on top of Theano.\n",
    "\n",
    "    3.  [Lasagne](https://github.com/Lasagne/Lasagne) is a lightweight library to build and train neural networks in Theano. It is governed by simplicity, transparency, modularity, pragmatism , focus and restraint principles.\n",
    "\n",
    "    4.  [Blocks](https://github.com/mila-udem/blocks) a framework that helps you build neural network models on top of Theano.\n",
    "\n",
    "2.  [Caffe](http://caffe.berkeleyvision.org/) is a deep learning framework made with expression, speed, and modularity in mind. It is developed by the Berkeley Vision and Learning Center (BVLC) and by community contributors. Google's [DeepDream](http://venturebeat.com/2015/07/01/google-open-sources-its-software-for-making-trippy-images-with-deep-learning/) is based on Caffe Framework. This framework is a BSD-licensed C++ library with Python Interface.\n",
    "\n",
    "3.  [_nolearn_](https://github.com/dnouri/nolearn) contains a number of wrappers and abstractions around existing neural network libraries, most notably [Lasagne](http://lasagne.readthedocs.org/), along with a few machine learning utility modules.\n",
    "\n",
    "4.  [Gensim](http://radimrehurek.com/gensim/) is deep learning toolkit implemented in python programming language intended for handling large text collections, using efficient algorithms.\n",
    "\n",
    "5.  [Chainer](http://chainer.org/) bridge the gap between algorithms and implementations of deep learning. Its powerful, flexible and intuitive and is considered as the [flexible framework](http://www.slideshare.net/beam2d/introduction-to-chainer-a-flexible-framework-for-deep-learning) for Deep Learning.\n",
    "\n",
    "6.  [deepnet](https://github.com/nitishsrivastava/deepnet) is a GPU-based python implementation of deep learning algorithms like Feed-forward Neural Nets, Restricted Boltzmann Machines, Deep Belief Nets, Autoencoders, Deep Boltzmann Machines and Convolutional Neural Nets.\n",
    "\n",
    "7.  [Hebel](https://github.com/hannes-brt/hebel) is a library for deep learning with neural networks in Python using GPU acceleration with CUDA through PyCUDA. It implements the most important types of neural network models and offers a variety of different activation functions and training methods such as momentum, Nesterov momentum, dropout, and early stopping.\n",
    "\n",
    "8.  [CXXNET](https://github.com/dmlc/cxxnet) is fast, concise, distributed deep learning framework based on MShadow. It is a lightweight and easy extensible C++/CUDA neural network toolkit with friendly Python/Matlab interface for training and prediction.\n",
    "\n",
    "9.  [DeepPy](https://github.com/andersbll/deeppy) is a Pythonic deep learning framework built on top of NumPy.\n",
    "\n",
    "10.  [DeepLearning](https://github.com/vishwa-raman/DeepLearning) is deep learning library, developed with C++ and python.\n",
    "\n",
    "11.  [Neon](https://github.com/NervanaSystems/neon) is Nervana's Python based Deep Learning framework.\n",
    "\n",
    "**Matlab**\n",
    "\n",
    "1.  [ConvNet](https://github.com/sdemyanov/ConvNet) Convolutional neural net is a type of deep learning classification algorithms, that can learn useful features from raw data by themselves and is performed by tuning its weighs.\n",
    "\n",
    "2.  [DeepLearnToolBox](https://github.com/rasmusbergpalm/DeepLearnToolbox) is a matlab/octave toolbox for deep learning and includes Deep Belief Nets, Stacked Autoencoders, convolutional neural nets.\n",
    "\n",
    "3.  [cuda-convnet](https://code.google.com/p/cuda-convnet/) is a fast C++/CUDA implementation of convolutional (or more generally, feed-forward) neural networks. It can model arbitrary layer connectivity and network depth. Any directed acyclic graph of layers will do. Training is done using the backpropagation algorithm.\n",
    "\n",
    "4.  [MatConvNet](http://www.vlfeat.org/matconvnet/ \"MatConvNet\") is a MATLAB toolbox implementing Convolutional Neural Networks (CNNs) for computer vision applications. It is simple, efficient, and can run and learn state-of-the-art CNNs.\n",
    "\n",
    "5.  [MatCaffe](http://caffe.berkeleyvision.org/) is a deep learning framework made with expression, speed, and modularity in mind. It is developed by the Berkeley Vision and Learning Center (BVLC) and by community contributors. Google's [DeepDream](http://venturebeat.com/2015/07/01/google-open-sources-its-software-for-making-trippy-images-with-deep-learning/) is based on Caffe Framework. This framework is a BSD-licensed C++ library with Python Interface.\n",
    "\n",
    "**CPP**\n",
    "\n",
    "1.  [eblearn](http://eblearn.sourceforge.net/index.shtml) is an open-source C++ library of machine learning by New York Universityâ€™s machine learning lab, led by Yann LeCun. In particular, implementations of convolutional neural networks with energy-based models along with a GUI, demos and tutorials.\n",
    "\n",
    "2.  [SINGA](http://www.comp.nus.edu.sg/~dbsystem/singa/) is designed to be general to implement the distributed training algorithms of existing systems. It is supported by Apache Software Foundation.\n",
    "\n",
    "3.  NVIDIA [DIGITS](https://developer.nvidia.com/digits) is a new system for developing, training and visualizing deep neural networks. It puts the power of deep learning into an intuitive browser-based interface, so that data scientists and researchers can quickly design the best DNN for their data using real-time network behavior visualization.\n",
    "\n",
    "4.  [IntelÂ® Deep Learning Framework](https://01.org/intel-deep-learning-framework) provides a unified framework for IntelÂ® platforms accelerating Deep Convolutional Neural Networks.\n",
    "\n",
    "**Java**\n",
    "\n",
    "1.  [N-Dimensional Arrays for Java](http://nd4j.org/) (ND4J)is scientific computing libraries for the JVM. They are meant to be used in production environments, which means routines are designed to run fast with minimum RAM requirements.\n",
    "\n",
    "2.  [Deeplearning4j](http://deeplearning4j.org/) is the first commercial-grade, open-source, distributed deep-learning library written for Java and Scala. It is designed to be used in business environments, rather than as a research tool.\n",
    "\n",
    "3.  [Encog](http://www.heatonresearch.com/encog) is an advanced machine learning framework which supports Support Vector Machines,Artificial Neural Networks, Genetic Programming, Bayesian Networks, Hidden Markov Models, Genetic Programming and Genetic Algorithms are supported.\n",
    "\n",
    "**JavaScript**\n",
    "\n",
    "1.  [Convnet.js](http://cs.stanford.edu/people/karpathy/convnetjs/) is a Javascript library for training Deep Learning models (mainly Neural Networks) entirely in a browser. No software requirements, no compilers, no installations, no GPUs, no sweat.\n",
    "\n",
    "**Lua**\n",
    "\n",
    "1.  [Torch](http://torch.ch/) is a scientific computing framework with wide support for machine learning algorithms. It is easy to use and efficient, fast scripting language, LuaJIT, and an underlying C/CUDA implementation. Torch is based on Lua programming language.\n",
    "\n",
    "**Julia**\n",
    "\n",
    "1.  [Mocha](https://github.com/pluskid/Mocha.jl) is a Deep Learning framework for Julia, inspired by the C++ framework Caffe. Efficient implementations of general stochastic gradient solvers and common layers in Mocha could be used to train deep / shallow (convolutional) neural networks, with (optional) unsupervised pre-training via (stacked) auto-encoders. Its best feature include Modular architecture, High-level Interface, portability with speed, compatibility and many more.\n",
    "\n",
    "**Lisp**\n",
    "\n",
    "1.  [Lush(Lisp Universal Shell)](http://lush.sourceforge.net/) is an object-oriented programming language designed for researchers, experimenters, and engineers interested in large-scale numerical and graphic applications. It comes with rich set of deep learning libraries as a part of machine learning libraries.\n",
    "\n",
    "**Haskell**\n",
    "\n",
    "1.  [DNNGraph](https://github.com/ajtulloch/dnngraph) is a deep neural network model generation DSL in Haskell.\n",
    "\n",
    "**.NET**\n",
    "\n",
    "1.  [Accord.NET](http://accord-framework.net/ \"Accord.NET\") is a .NET machine learning framework combined with audio and image processing libraries completely written in C#. It is a complete framework for building production-grade computer vision, computer audition, signal processing and statistics applications\n",
    "\n",
    "**R**\n",
    "\n",
    "1.  [darch](http://cran.um.ac.ir/web/packages/darch/index.html \"darch\") package can be used for generating neural networks with many layers (deep architectures). Training methods includes a pre training with the contrastive divergence method and a fine tuning with common known training algorithms like backpropagation or conjugate gradient.\n",
    "2.  [deepnet](https://cran.r-project.org/web/packages/deepnet/index.html \"deepnet\") implements some deep learning architectures and neural network algorithms, including BP,RBM,DBN,Deep autoencoder and so on.\n",
    "\n",
    "\n",
    "\n",
    "source [http://www.teglor.com/b/deep-learning-libraries-language-cm569/](http://www.teglor.com/b/deep-learning-libraries-language-cm569/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convnet-benchmarks\n",
    "---\n",
    "\n",
    "Easy benchmarking of all public open-source implementations of convnets.\n",
    "A summary is provided in the section below.\n",
    "\n",
    "Machine: `6-core Intel Core i7-5930K CPU @ 3.50GHz` + `NVIDIA Titan X` + `Ubuntu 14.04 x86_64`\n",
    "\n",
    "## Imagenet Winners Benchmarking\n",
    "Time for a full forward + backward pass. I average my times over 10 runs. Ignoring dropout and softmax layers.\n",
    "\n",
    "**[AlexNet (One Weird Trick paper)](https://code.google.com/p/cuda-convnet2/source/browse/layers/layers-imagenet-1gpu.cfg)** - Input 128x3x224x224\n",
    "\n",
    "| Library         | Class                                                                                                                | Time (ms)  | forward (ms) | backward (ms) |\n",
    "|:------------------------:|:-----------------------------------------------------------------------------------------------------------:| ----------:| ------------:| -------------:|\n",
    "| **Nervana-fp16**    | [ConvLayer](https://github.com/soumith/convnet-benchmarks/blob/master/nervana/README.md)                    |   **92**   |  **29**      |    **62**     |\n",
    "| CuDNN[R3]-fp16      | [cudnn.SpatialConvolution](https://github.com/soumith/cudnn.torch/blob/master/SpatialConvolution.lua)       |      96    |  30          |   66          |\n",
    "| CuDNN[R3]-fp32      | [cudnn.SpatialConvolution](https://github.com/soumith/cudnn.torch/blob/master/SpatialConvolution.lua)       |      96    |  32          |   64          |\n",
    "| Nervana-fp32        | [ConvLayer](https://github.com/soumith/convnet-benchmarks/blob/master/nervana/README.md)                    |      101   |  32          |    69         |\n",
    "| fbfft                    | [fbnn.SpatialConvolution](https://github.com/facebook/fbcunn/tree/master/src/fft)                           |      104   |  31          |    72         |\n",
    "| cudaconvnet2*            | [ConvLayer](https://github.com/soumith/cuda-convnet2.torch/blob/master/cudaconv3/src/filter_acts.cu)        |      177   |  42          |   135         |\n",
    "| CuDNN[R2] *             | [cudnn.SpatialConvolution](https://github.com/soumith/cudnn.torch/blob/master/SpatialConvolution.lua)       |      231   |  70          |   161         |\n",
    "| Caffe (native)           | [ConvolutionLayer](https://github.com/BVLC/caffe/blob/master/src/caffe/layers/conv_layer.cu)                |      324   | 121          |   203         |\n",
    "| Torch-7 (native)         | [SpatialConvolutionMM](https://github.com/torch/cunn/blob/master/SpatialConvolutionMM.cu)                   |      342   | 132          |   210         |\n",
    "| CL-nn (Torch)            | [SpatialConvolutionMM](https://github.com/hughperkins/clnn/blob/master/SpatialConvolutionMM.cl)             |      963   | 388          |   574         |\n",
    "| Caffe-CLGreenTea         | [ConvolutionLayer](https://github.com/naibaf7/caffe)             |      1442   | 210          |   1232         |\n",
    "\n",
    "**[Overfeat [fast]](http://arxiv.org/abs/1312.6229)** - Input 128x3x231x231\n",
    "\n",
    "| Library                  | Class                                                                                                                    | Time (ms)         | forward (ms)            | backward (ms)            |\n",
    "|:------------------------:|:------------------------------------------------------------------------------------------------------------------------:| -----------------:| -----------------------:| ------------------------:|\n",
    "| **CuDNN[R3]-fp16**       | [cudnn.SpatialConvolution](https://github.com/soumith/cudnn.torch/blob/master/SpatialConvolution.lua)                    |         **313**       |  **107**                    |  **206**             |\n",
    "| CuDNN[R3]-fp32       | [cudnn.SpatialConvolution](https://github.com/soumith/cudnn.torch/blob/master/SpatialConvolution.lua)                    |         326       |  113                    |   213                    |\n",
    "| fbfft                    | [SpatialConvolutionCuFFT](https://github.com/facebook/fbcunn/tree/master/src/fft)                                        |         342       |  114                    |   227                    |\n",
    "| Nervana-fp16          | [ConvLayer](https://github.com/soumith/convnet-benchmarks/blob/master/nervana/README.md)                                 |         355       |  112                    |   242                    |\n",
    "| Nervana-fp32            | [ConvLayer](https://github.com/soumith/convnet-benchmarks/blob/master/nervana/README.md)                                 |         398       |  124                    |   273                    |\n",
    "| cudaconvnet2*            | [ConvLayer](https://github.com/soumith/cuda-convnet2.torch/blob/master/cudaconv3/src/filter_acts.cu)                     |         723       |  176                    |   547                    |\n",
    "| CuDNN[R2] *             | [cudnn.SpatialConvolution](https://github.com/soumith/cudnn.torch/blob/master/SpatialConvolution.lua)                    |         810       |  234                    |   576                    |\n",
    "| Caffe                    | [ConvolutionLayer](https://github.com/BVLC/caffe/blob/master/src/caffe/layers/conv_layer.cu)                             |         823       |  355                    |   468                    |\n",
    "| Torch-7 (native)         | [SpatialConvolutionMM](https://github.com/torch/cunn/blob/master/SpatialConvolutionMM.cu)                                |         878       |  379                    |   499                    |\n",
    "| CL-nn (Torch)            | [SpatialConvolutionMM](https://github.com/hughperkins/clnn/blob/master/SpatialConvolutionMM.cl)                          |         963       |  388                    |   574                    |\n",
    "| Caffe-CLGreenTea         | [ConvolutionLayer](https://github.com/naibaf7/caffe)             |      2857   | 616          |   2240         |\n",
    "\n",
    "**[OxfordNet [Model-A]](http://arxiv.org/abs/1409.1556/)** - Input 64x3x224x224\n",
    "\n",
    "| Library                  | Class                                                                                                                    | Time (ms)         | forward (ms)            | backward (ms)            |\n",
    "|:------------------------:|:------------------------------------------------------------------------------------------------------------------------:| -----------------:| -----------------------:| ------------------------:|\n",
    "| **Nervana-fp16**    | [ConvLayer](https://github.com/soumith/convnet-benchmarks/blob/master/nervana/README.md)                                 |    **529**        |  **167**                |   **362**                |\n",
    "| Nervana-fp32        | [ConvLayer](https://github.com/soumith/convnet-benchmarks/blob/master/nervana/README.md)                                 |        590        |  180                    |   410                    |\n",
    "| CuDNN[R3]-fp16      | [cudnn.SpatialConvolution](https://github.com/soumith/cudnn.torch/blob/master/SpatialConvolution.lua)                    |       615         |  179                    |   436                    |\n",
    "| CuDNN[R3]-fp32      | [cudnn.SpatialConvolution](https://github.com/soumith/cudnn.torch/blob/master/SpatialConvolution.lua)                    |       615         |  196                    |   418                    |\n",
    "| fbfft                    | [SpatialConvolutionCuFFT](https://github.com/facebook/fbcunn/tree/master/src/fft)                                        |       1092        |  355                    |   737                    |\n",
    "| cudaconvnet2*            | [ConvLayer](https://github.com/soumith/cuda-convnet2.torch/blob/master/cudaconv3/src/filter_acts.cu)                     |       1229        |  408                    |   821                    |\n",
    "| CuDNN[R2] *             | [cudnn.SpatialConvolution](https://github.com/soumith/cudnn.torch/blob/master/SpatialConvolution.lua)                    |       1099        |  342                    |   757                    |\n",
    "| Caffe                    | [ConvolutionLayer](https://github.com/BVLC/caffe/blob/master/src/caffe/layers/conv_layer.cu)                             |       1068        |  323                    |   745                    |\n",
    "| Torch-7 (native)         | [SpatialConvolutionMM](https://github.com/torch/cunn/blob/master/SpatialConvolutionMM.cu)                                |       1105        |  350                    |   755                    |\n",
    "| CL-nn (Torch)            | [SpatialConvolutionMM](https://github.com/hughperkins/clnn/blob/master/SpatialConvolutionMM.cl)                          |       3437        |  875                    |   2562                   |\n",
    "| Caffe-CLGreenTea         | [ConvolutionLayer](https://github.com/naibaf7/caffe)             |      5620   | 988          |   4632         |\n",
    "\n",
    "**[GoogleNet V1](http://research.google.com/pubs/pub43022.html)** - Input 128x3x224x224\n",
    "\n",
    "| Library                  | Class                                                                                                                    | Time (ms)         | forward (ms)            | backward (ms)            |\n",
    "|:------------------------:|:------------------------------------------------------------------------------------------------------------------------:| -----------------:| -----------------------:| ------------------------:|\n",
    "| **Nervana-fp16**    | [ConvLayer](https://github.com/soumith/convnet-benchmarks/blob/master/nervana/README.md)                                 |    **283**        |  **85**                 |   **197**                |\n",
    "| Nervana-fp32        | [ConvLayer](https://github.com/soumith/convnet-benchmarks/blob/master/nervana/README.md)                                 |        322        |  90                     |   232                    |\n",
    "| CuDNN[R3]-fp32       | [cudnn.SpatialConvolution](https://github.com/soumith/cudnn.torch/blob/master/SpatialConvolution.lua)                    |       431         |  117                    |   313                    |\n",
    "| CuDNN[R3]-fp16       | [cudnn.SpatialConvolution](https://github.com/soumith/cudnn.torch/blob/master/SpatialConvolution.lua)                    |       501         |  109                    |   392                    |\n",
    "| Caffe                    | [ConvolutionLayer](https://github.com/BVLC/caffe/blob/master/src/caffe/layers/conv_layer.cu)                             |       1935        |  786                    |   1148                   |\n",
    "| CL-nn (Torch)            | [SpatialConvolutionMM](https://github.com/hughperkins/clnn/blob/master/SpatialConvolutionMM.cl)                          |       7016        |  3027                   |   3988                   |\n",
    "| Caffe-CLGreenTea         | [ConvolutionLayer](https://github.com/naibaf7/caffe)             |      9462   | 746          |   8716         |\n",
    "\n",
    "source [https://github.com/soumith/convnet-benchmarks](https://github.com/soumith/convnet-benchmarks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# The Big 3\n",
    "---\n",
    "**Caffe**\n",
    "- C++/YAML with python and matlab interfaces\n",
    "- Only can implement CNN's\n",
    "- Huge community and active community\n",
    "- Most computer vision research uses Caffe\n",
    "- Can implement your own layers in c++/cuda/python\n",
    "- Colaberates with Nvidia, first to use new CudNN libraries\n",
    "- Have to define network structure in YAML\n",
    "- OSX and Linux\n",
    "\n",
    "**Theano**\n",
    "- Python library\n",
    "- Built around the Scypy/Numpy evironment\n",
    "- General Machine Learning Library\n",
    "- Outside of computer vision, most used\n",
    "- Define net structure in python\n",
    "- Very low level, but has a lot of useful high level libraries \n",
    "    - NoLearn\n",
    "    - Lasagne\n",
    "- OSX, Linux, and Windows\n",
    "\n",
    "**Torch**\n",
    "- Lua Library\n",
    "- Used by Google and Facebook\n",
    "- Very low level\n",
    "- OSX and Linux\n",
    "\n",
    "\n",
    "\n",
    "## My advice, USE UBUNTU 14.04 LTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What GPU Do I Want?\n",
    "---\n",
    "|   GPU Name  |  Cost | Memory |   Power Requirement  | Single Precision Speed |\n",
    "|:-----------:|:-----:|:------:|:--------------------:|:----------------------:|\n",
    "|  GTX 750ti  |  \\$150 |   2GB  |     55w - No PCIe    |       1306 GFLOPS      |\n",
    "|   GTX 970   |  \\$330 |  3.5GB |    145w - 2x 6-Pin   |       3494 GFLOPS      |\n",
    "|  GTX 980ti  |  \\$650 |   6GB  | 250w - 6-Pin + 8-Pin |       5632 GFLOPS      |\n",
    "| GTX Titan X | \\$1000 |  12GB  | 250w - 6-Pin + 8-Pin |       6144 GLOPS       |\n",
    "| GTX Titan Z | \\$1550 |  6GBx2 |   375w - 2x 8-Pin    |     4061 GFLOPS x2     |\n",
    "\n",
    "\n",
    "## Tesla? AMD? Titan Black?\n",
    "- CNN and nural net libraries all use CUDA as thier back end and only depend on 32 bit single percision (or 16 bit half) accuracy.\n",
    "- AMD cards do not support CUDA\n",
    "- Tesla GPU's and older Nvidia GPU's (like the Titan, Titan Black, and 700 series) use older architecture that focus more on 64 bit double percision performance, and have poor 32 bit performance for the cost. \n",
    "    - The GTX 750ti is an exception as it uses the newer architecture\n",
    "    - The Titan Z is only useful if you need to fit 2 GPU's in a computer with only 1 open PCIe slot. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can My computer use the GPU?\n",
    "---\n",
    "### An open PCIe x16 slot\n",
    "![pcie slots](https://upload.wikimedia.org/wikipedia/commons/0/0c/PCI_und_PCIe_Slots.jpg)\n",
    "\n",
    "### A Power supply with enough wattage and PCIe power cables\n",
    "![8 and 8 pin connectors](http://cdn.head-fi.org/5/56/56ff61d3_pcie-connectors.jpeg)\n",
    "![powersupply sticker](http://www.techpowerup.com/reviews/Corsair/GS800/images/psu_label.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo Time\n",
    "[Source for this demo](http://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/)\n",
    "\n",
    "\n",
    "## Libraries I am using\n",
    "[Theano](http://deeplearning.net/software/theano/) + [Lasagne](https://github.com/Lasagne/Lasagne) + [NoLearn](https://github.com/dnouri/nolearn)\n",
    "\n",
    "- Installing NoLearn will install the other 2 libraries for you from its requirements.txt file. \n",
    "\n",
    "\n",
    "## Data\n",
    "\n",
    "#### Facial Keypoints Detection (Kaggle)\n",
    "![face](https://kaggle2.blob.core.windows.net/competitions/kaggle/3486/logos/front_page.png)\n",
    "\n",
    "\n",
    "- The objective of this task is to predict keypoint positions on face images. \n",
    "- [Download the training and test data from here](https://www.kaggle.com/c/facial-keypoints-detection/data)\n",
    "\n",
    "\n",
    "\n",
    "#### First define a function to Load in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from pandas.io.parsers import read_csv\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "FTRAIN = 'training.csv'\n",
    "FTEST = 'test.csv'\n",
    "\n",
    "\n",
    "def load(test=False, cols=None):\n",
    "    \"\"\"Loads data from FTEST if *test* is True, otherwise from FTRAIN.\n",
    "    Pass a list of *cols* if you're only interested in a subset of the\n",
    "    target columns.\n",
    "    \"\"\"\n",
    "    fname = FTEST if test else FTRAIN\n",
    "    df = read_csv(os.path.expanduser(fname))  # load pandas dataframe\n",
    "\n",
    "    # The Image column has pixel values separated by space; convert\n",
    "    # the values to numpy arrays:\n",
    "    df['Image'] = df['Image'].apply(lambda im: np.fromstring(im, sep=' '))\n",
    "\n",
    "    if cols:  # get a subset of columns\n",
    "        df = df[list(cols) + ['Image']]\n",
    "\n",
    "    print(df.count())  # prints the number of values for each column\n",
    "    df = df.dropna()  # drop all rows that have missing values in them\n",
    "\n",
    "    X = np.vstack(df['Image'].values) / 255.  # scale pixel values to [0, 1]\n",
    "    X = X.astype(np.float32)\n",
    "\n",
    "    if not test:  # only FTRAIN has any target columns\n",
    "        y = df[df.columns[:-1]].values\n",
    "        y = (y - 48) / 48  # scale target coordinates to [-1, 1]\n",
    "        X, y = shuffle(X, y, random_state=42)  # shuffle train data\n",
    "        y = y.astype(np.float32)\n",
    "    else:\n",
    "        y = None\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in said data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y = load()\n",
    "print(\"X.shape == {}; X.min == {:.3f}; X.max == {:.3f}\".format(\n",
    "    X.shape, X.min(), X.max()))\n",
    "print(\"y.shape == {}; y.min == {:.3f}; y.max == {:.3f}\".format(\n",
    "    y.shape, y.min(), y.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show a sample of the Data and its ground truth points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as pyplot\n",
    "%matplotlib inline \n",
    "\n",
    "def plot_sample(x, y, axis):\n",
    "    img = x.reshape(96, 96)\n",
    "    axis.imshow(img, cmap='gray')\n",
    "    axis.scatter(y[0::2] * 48 + 48, y[1::2] * 48 + 48, marker='x', s=10)\n",
    "    \n",
    "fig = pyplot.figure(figsize=(6, 6))\n",
    "fig.subplots_adjust(\n",
    "    left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "\n",
    "for i in range(16):\n",
    "    ax = fig.add_subplot(4, 4, i + 1, xticks=[], yticks=[])\n",
    "    plot_sample(X[i], y[i], ax)\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in the needed Python libraries to run our CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lasagne import layers\n",
    "from lasagne.updates import nesterov_momentum\n",
    "from nolearn.lasagne import NeuralNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First lets define a really basic single layer neural net to train as a baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net = NeuralNet(\n",
    "    layers=[  # three layers: one hidden layer\n",
    "        ('input', layers.InputLayer),\n",
    "        ('hidden', layers.DenseLayer),\n",
    "        ('output', layers.DenseLayer),\n",
    "        ],\n",
    "    # layer parameters:\n",
    "    input_shape=(None, 9216),  # 96x96 input pixels per batch\n",
    "    hidden_num_units=100,  # number of units in hidden layer\n",
    "    output_nonlinearity=None,  # output layer uses identity function\n",
    "    output_num_units=30,  # 30 target values\n",
    "\n",
    "    # optimization method:\n",
    "    update=nesterov_momentum, # <-------\n",
    "    update_learning_rate=0.01,\n",
    "    update_momentum=0.9,\n",
    "\n",
    "    regression=True,  # flag to indicate we're dealing with regression problem\n",
    "    max_epochs=400,  # we want to train this many epochs\n",
    "    verbose=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What to take note of here\n",
    "- How the 3 layers are defined and the parameters for each.\n",
    "- The optimization method we have chosen\n",
    "\n",
    "\n",
    "##### Other possible optimization methods\n",
    "![Optimization methods](http://i.imgur.com/s25RsOr.gif)\n",
    "\n",
    "\n",
    "#### Now we actualy train our basic net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output = net.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets estimate our MSE to compare with the kaggel leader board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.sqrt(net.train_history_[-1][\"valid_loss\"]) * 48 # we scaled our cordinates between -1 and 1 so we have to reverse it by multiplying by 48. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the loss over the number of training epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_loss = np.array([i[\"train_loss\"] for i in net.train_history_])\n",
    "valid_loss = np.array([i[\"valid_loss\"] for i in net.train_history_])\n",
    "pyplot.plot(train_loss, linewidth=3, label=\"train\")\n",
    "pyplot.plot(valid_loss, linewidth=3, label=\"valid\")\n",
    "pyplot.grid()\n",
    "pyplot.legend()\n",
    "pyplot.xlabel(\"epoch\")\n",
    "pyplot.ylabel(\"loss\")\n",
    "pyplot.ylim(1e-3, 1e-2)\n",
    "pyplot.yscale(\"log\")\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the performance on some of the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, _ = load(test=True)\n",
    "y_pred = net.predict(X)\n",
    "\n",
    "fig = pyplot.figure(figsize=(6, 6))\n",
    "fig.subplots_adjust(\n",
    "    left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "\n",
    "for i in range(16):\n",
    "    ax = fig.add_subplot(4, 4, i + 1, xticks=[], yticks=[])\n",
    "    plot_sample(X[i], y_pred[i], ax)\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Switch to a CNN\n",
    "---\n",
    "\n",
    "#### First redefine the input as a 2d matrix instead od a 1d vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load2d(test=False, cols=None):\n",
    "    X, y = load(test=test)\n",
    "    X = X.reshape(-1, 1, 96, 96)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Redefine the neural net to use convolution and pooling layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net2 = NeuralNet(\n",
    "    layers=[\n",
    "        ('input', layers.InputLayer),\n",
    "        ('conv1', layers.Conv2DLayer),     # 3 sets of convolution into pooling layers added to the network\n",
    "        ('pool1', layers.MaxPool2DLayer),\n",
    "        ('conv2', layers.Conv2DLayer),\n",
    "        ('pool2', layers.MaxPool2DLayer),\n",
    "        ('conv3', layers.Conv2DLayer),\n",
    "        ('pool3', layers.MaxPool2DLayer),\n",
    "        ('hidden4', layers.DenseLayer),\n",
    "        ('hidden5', layers.DenseLayer),   # added a second fully connected hidden layer\n",
    "        ('output', layers.DenseLayer),\n",
    "        ],\n",
    "    input_shape=(None, 1, 96, 96), # notice the difference in the input shape\n",
    "    conv1_num_filters=32, conv1_filter_size=(3, 3), pool1_pool_size=(2, 2), # This is where we define the paramaters for the \n",
    "    conv2_num_filters=64, conv2_filter_size=(2, 2), pool2_pool_size=(2, 2), # convolution layers and their respective pooling\n",
    "    conv3_num_filters=128, conv3_filter_size=(2, 2), pool3_pool_size=(2, 2),# layers. \n",
    "    hidden4_num_units=500, hidden5_num_units=500,\n",
    "    output_num_units=30, output_nonlinearity=None,\n",
    "\n",
    "    update_learning_rate=0.01, # the same update algorithm as last time\n",
    "    update_momentum=0.9,\n",
    "\n",
    "    regression=True,\n",
    "    max_epochs=1000, # training for much longer\n",
    "    verbose=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And lets train again (will take much longer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y = load2d()  # load 2-d data\n",
    "output = net2.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in a pretrain version so we dont have to wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "#with open('net2.pickle', 'wb') as f: # this is for saving the trained net.\n",
    "#    pickle.dump(net2, f, -1)\n",
    "\n",
    "net2 = pickle.load(open('net2.pickle', 'r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets estimate our MSE to compare with the kaggel leader board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.sqrt(net2.train_history_[-1][\"valid_loss\"]) * 48"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the loss over the number of training epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_loss = np.array([i[\"train_loss\"] for i in net.train_history_])\n",
    "valid_loss = np.array([i[\"valid_loss\"] for i in net.train_history_])\n",
    "pyplot.plot(train_loss, linewidth=3, label=\"train\")\n",
    "pyplot.plot(valid_loss, linewidth=3, label=\"valid\")\n",
    "pyplot.grid()\n",
    "pyplot.legend()\n",
    "pyplot.xlabel(\"epoch\")\n",
    "pyplot.ylabel(\"loss\")\n",
    "pyplot.ylim(1e-3, 1e-2)\n",
    "pyplot.yscale(\"log\")\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the performance on some of the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample1 = load(test=True)[0][6:7]\n",
    "sample2 = load2d(test=True)[0][6:7]\n",
    "y_pred1 = net.predict(sample1)[0]\n",
    "y_pred2 = net2.predict(sample2)[0]\n",
    "\n",
    "fig = pyplot.figure(figsize=(6, 3))\n",
    "ax = fig.add_subplot(1, 2, 1, xticks=[], yticks=[])\n",
    "plot_sample(sample1[0], y_pred1, ax)\n",
    "ax = fig.add_subplot(1, 2, 2, xticks=[], yticks=[])\n",
    "plot_sample(sample1[0], y_pred2, ax)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
